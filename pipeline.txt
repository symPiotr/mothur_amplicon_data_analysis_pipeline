#######################################
############## OUR DATA ###############

>>> 230ish amplicon libraries, for different genes (insect mitochondrial - COI, 16S rRNA; bacterial - 16S rRNA, different regions), for different insects or groups of insects. Different amplicon types within library.
>>> Libraries prepared using custom two-step PCR (first round - template-specific PCR; second round - indexing), following a modified Meyer & Kircher 2010 protocol.
>>> Pooled libraries sequenced on a shared MiSeq v3 lane (2x300bp reads) at the Institute of Environmental Sciences
>>> Data available on the IES cluster: /mnt/matrix/symbio/raw_data/20191109_MiSeq_first_run/

#######################################
####### THE PIPELINE - SUMMARY ########

For selected samples, we:
	Copy compressed reads to a working directory, unpack them
	If needed, trim reads using trim_galore
	Assemble reads into contigs
	Quality-screen reads and trim primers
	Select unique sequences and compute count table
	Discard rare unique sequences
	Align reads to a reference, discard those that didn't align
	[16S] Screen for chimeras
	[16S] Assign taxonomy
	Rename sequences - simplify names!
	Compute distance matrix between sequences, pick OTUs
	Prepare OTU and summary tables

Example data that I am starting with: three libraries for Macrosteles laevis leafhopper individuals, prepped by Sandra and Monika. They attempted to amplify three genes:
	Insect mitochondrial COI (primers BF3 & BR2)
	Insect mitochondrial 16S rRNA gene, v4 region (primers Chiar16SF and Chiar16SR, Marquina et al. 2018)
	Bacterial 16S rRNA (primers 515F and 806R)
... but mito16S didn't amplify because of a critical mismatch in one of the primers. Then, these libraries contain the mixture of two products.

In this particular example, reads (300 bp) are longer than the 16S-v4 region incl. primers (~292 bp). To avoid complications down the line, we may trim them. 


#######################################
######## SETTING UP WORKSPACE #########

### Login to the cluster!
ssh user.name@149.156.165.82			# <- fsm cluster. More memory, processors and hard disk space.
# ... or ...
ssh user.name@149.156.165.80 			# <- leskowiec cluster.


### Create working directory, perhaps in your home directory. Go there.
cd ~
mkdir amplicon_analyses
cd amplicon_analyses
mkdir 20191112_test
cd 20191112_test

### Select files for analysis.
### Mothur can work with gzip-compressed files, but the last time I tried that I had some issues.
### Hence, you might choose to copy selected files, for example to raw_reads folder in your work-dir, and uncompress them there.
### You may or may not want to rename the files.

mkdir raw_reads
cd raw_reads

cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW1_S213_L001_R1_001.fastq.gz ./BTW1_R1.fastq.gz
cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW1_S213_L001_R2_001.fastq.gz ./BTW1_R2.fastq.gz
cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW2_S210_L001_R1_001.fastq.gz ./BTW2_R1.fastq.gz
cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW2_S210_L001_R2_001.fastq.gz ./BTW2_R2.fastq.gz
cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW3_S207_L001_R1_001.fastq.gz ./BTW3_R1.fastq.gz
cp /home/piotr.lukasik/symbio/raw_data/20191109_MiSeq_first_run/S-BTW3_S207_L001_R2_001.fastq.gz ./BTW3_R2.fastq.gz

gunzip *

### If your reads are longer than some of your target amplicons, as in our example (300 bp reads, 292 bp 16S-v4 amplicons), it may be a good idea to trim the reads.
### Removing the last 50 bases of reads, which tend to be low quality anyway, will help us avoid downstream problems.

/mnt/matrix/symbio/bin/trim_galore --dont_gzip --hardtrim5 250 *fastq
    # that should work from any location: you are specifying absolute path to the executable.
    # OR if you have added /mnt/matrix/symbio/bin to your PATH, this should work ---
trim_galore --dont_gzip --hardtrim5 250 *fastq

### Check cluster usage. Make sure it's not being used too heavily!
### My favorite tool is htop:
htop
    # Press q to quit!

### Start screen --- an interactive session which helps ensure that a broken network connection won't interrupt your analyses
screen -S mothur_session
    # Other useful commands:
	# Ctrl+a, Ctrl+d --- leave session;
	# screen -r <session_name OR session#> --- rejoin session;
	# screen -rd <session_name OR session#> --- rejoin session which is listed as "attached", for example because the software hasn't yet realized that you lost connection ;
	# screen -ls --- list running sessions;
	# Ctrl+a, then type :quit --- kill session you are connected to.



#######################################
#### MOTHUR - SETUP, QC, TRIMMING #####

### Within a screen, move to your working directory
### Then, start mothur!
mothur

### List the current working directories. This is important --- this is where the software
### will be looking for input files and saving output files! You can change the directory at any point
### using the next command.
get.current()

### Set working directory... initially for the folder with reads:
set.dir(input=~/amplicon_analyses/20191112_test/raw_reads, output=~/amplicon_analyses/20191112_test/raw_reads)

### Assemble forward and reverse reads into contigs
### First, create the file with a list of libraries and files.
make.file(inputdir=~/amplicon_analyses/20191112_test/raw_reads, type=fastq, prefix=ML)

### You probably want to see how the file looks, and edit names, remove some of the libraries, etc.
system(cat ~/amplicon_analyses/20191112_test/raw_reads/ML.files)
    # Hopefully, lines will go like this ---
        # BTW1	BTW1_R1.fastq	BTW1_R2.fastq
    # You may want to edit your files manually to avoid dashes within sample names, as this would create problems downstream. You may want to change library names, remove some libraries, add some libraries...
    # In particular, if you have trimmed and pre-trimmed reads in the same folder, you may want to make sure that the libraries are linked to the trimmed fastq files:

### Change working directory, so that any subsequent files are written into your work_dir:
set.dir(input=~/amplicon_analyses/20191112_test, output=~/amplicon_analyses/20191112_test)

### Join forward and reverse reads into contigs!
make.contigs(file=/home/piotr.lukasik/amplicon_analyses/20191112_test/raw_reads/ML.files)
    # Output File Names: 
	# /home/piotr.lukasik/amplicon_analyses/20191112_test/ML.trim.contigs.fasta
	# /home/piotr.lukasik/amplicon_analyses/20191112_test/ML.scrap.contigs.fasta
	# /home/piotr.lukasik/amplicon_analyses/20191112_test/ML.contigs.report
	# /home/piotr.lukasik/amplicon_analyses/20191112_test/ML.contigs.groups
    # let's look at them! Make sure that you do understand the organization of the fasta file and the group file!

### Let's now disconnect from the screen where mothur runs...
    # Ctrl+a+d

### And look at a subset of original reads and the assembled contigs. 
head -20 raw_reads/S-BTW1_R1.fastq
    # displays the first five R1 reads ...
head -20 raw_reads/S-BTW1_R2.fastq
    # displays the first five R2 reads ...
head -20 ML.trim.contigs.fasta
    # displays the first five contigs.

### Paste reads into CodonCode Aligner, assemble reads + contigs corresponding to the same read pair ... Make sure that you understand what is what, and that it all makes sense!

### Display the summary of sequences
summary.seqs(fasta=ML.trim.contigs.fasta)
    # or alternatively, at any point you can refer to the last saved file of any given type, "current"
    # ---> as indicated by get.current()
summary.seqs(fasta=current)
   
### See how many sequences there are in each library
count.groups(group=ML.contigs.groups)
    # or alternatively,
count.groups(group=current)


#######################################
##### MOTHUR - CONTIG PROCESSING ######

### Quality-trimming sequences
trim.seqs(fasta=ML.trim.contigs.fasta, oligos=/mnt/matrix/symbio/db/references/primers_to_trim.oligos, minlength=250, maxlength=500, maxambig=0, maxhomop=10, pdiffs=2)
    # Lots of parameters! You may want to google "mothur trim.seqs" to understand them!
    # The important file here is the list of primers to trim, primers_to_trim.oligos. This is how it looks:
	####### COI
	####### Our oligos used for the library preps:
	#COIBF3_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT---CCHGAYATRGCHTTYCCHCG
	#COIBR2_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT---TCDGGRTGNCCRAARAAYCA

	forward CCHGAYATRGCHTTYCCHCG
	reverse TCDGGRTGNCCRAARAAYCA


	###### mitochondrial 16S rRNA
	####### Our oligos used for the library preps:
	#Chiar16SR_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT---CYGTRCDAAGGTAGCATA
	#Chiar16SF_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT---ARTYCAACATCGRGGTC

	forward CYGTRCDAAGGTAGCAWA
	reverse ARTYCAACATCGRGGTC


	###### bacterial 16S rRNA
	####### Our oligos used for the library preps:
	# 515F_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT---GTGYCAGCMGCCGCGGTAA
	# 806R_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT---GGACTACNVGGGTWTCTAAT

	forward GTGYCAGCMGCCGCGGTAA
	reverse GGACTACNVGGGTWTCTAAT


### Listing sequences retained in trimmed fasta file, and extracting them from the group file:
list.seqs(fasta=current)
get.seqs(accnos=current, group=current)
    # or alternatively,
list.seqs(fasta=ML.trim.contigs.trim.fasta)
get.seqs(accnos=ML.trim.contigs.trim.accnos, group=ML.contigs.groups)

### Get basic info about the sequences!
summary.seqs(fasta=ML.trim.contigs.trim.fasta)

### Check how the read numbers in libraries have changed!
count.groups(group=ML.contigs.groups)
    # before trimming...
count.groups(group=ML.contigs.pick.groups)
    # after trimming...
   
### Ensure that the number of removed reads is reasonable --- or consider adjusting your parameters!

### OPTIONAL: if you are dealing with many files, you might want to select libraries for analysis from larger datasets
# get.groups(group=ML.contigs.pick.groups, fasta=ML.trim.contigs.trim.fasta, groups=PL301-PL302-TETLON-...)

### OPTIONAL: at any point, you might choose to merge files from different analyses
# merge.files(input=samples_first_run.trim.contigs.trim.fasta-samples_second_run.trim.contigs.trim.fasta, output=populations.fasta)
# merge.files(input=samples_first_run.contigs.pick.groups-samples_second_run.trim.contigs.trim.groups, output=populations.groups)


### Pick unique sequences
### This is an important step! Google "mothur unique.seqs"?
unique.seqs(fasta=ML.trim.contigs.trim.fasta)

### Generate count_table = a unique sequence table
### This is an important step! Google?
count.seqs(name=ML.trim.contigs.trim.names, group=ML.contigs.pick.groups, compress=f)

### Check the sequences, with and without group file!
summary.seqs(fasta=ML.trim.contigs.trim.unique.fasta)
summary.seqs(fasta=ML.trim.contigs.trim.unique.fasta, count=ML.trim.contigs.trim.count_table)

### Count sequences in each library, this time using the count_table as input
count.groups(count=ML.trim.contigs.trim.count_table)

### I encourage dicarding unique genotypes present only once in the dataset (singletons)
split.abund(fasta=ML.trim.contigs.trim.unique.fasta, count=ML.trim.contigs.trim.count_table, cutoff=1)

### Get sequence and library stats!
summary.seqs(fasta=current, count=current)
count.groups(count=current)



####################################################
### MOTHUR: COI data - alignments, OTU picking.. ###

### First, let's try to extract M. laevis COI sequences

### Align sequences against a complete MLAE mitogenome!
### That might take a while.
align.seqs(fasta=ML.trim.contigs.trim.unique.abund.fasta, reference=/mnt/matrix/symbio/db/references/mito_MLAE.fasta, processors=16)
   # COI alignment region expected: 2578-2995
   # 16S alignment region expected: 13487-1382
   
   # Consider using other references:
   	/mnt/matrix/symbio/db/references/mito16S_MACQUA.fasta (mitochondrial 16S rRNA of Macrosteles quadrilineatus)
	/mnt/matrix/symbio/db/references/COI_MACQUA.fasta (mitochondrial COI of Macrosteles quadrilineatus)

### Now, let's see how the sequences have aligned
summary.seqs(fasta=ML.trim.contigs.trim.unique.abund.align)

### Let's remove those unaligned
screen.seqs(fasta=ML.trim.contigs.trim.unique.abund.align, count=ML.trim.contigs.trim.abund.count_table, start=2578, end=2995, minlength=400)

summary.seqs(fasta=current, count=current)
count.groups(count=current)

### Filter.seqs will remove all empty (dash-only) columns from the alignment fasta file
filter.seqs(fasta=current, vertical=T, trump=.)

######## It might be helpful to now simplify file names!
rename.file(fasta=current, count=current, prefix=ML_COI)


### Finally, OTU picking for these COI sequences
### Compute distance matrix across sequences
### Then, cluster!
### Then, construct summary lists and tables!
dist.seqs(fasta=ML_COI.fasta, processors=16, cutoff=0.05)
cluster(column=current, count=ML_COI.count_table, cutoff=0.03, method=opti)
bin.seqs(list=current, fasta=current, label=0.01)
    # Assigning sequences to OTUs based on 99% identity
make.shared(list=current,count=current,label=0.01)
    # Create a 99% OTU table.

### Look at the resulting files!

### Get a representative sequence for each OTU!
### The command is not working at the moment - TO BE FIXED!
# get.oturep(column=current, list=current, fasta=current, cutoff=0.01)




####################################################
### MOTHUR: 16S data - alignments, OTU picking.. ###

### Align 16S sequences against a SILVA db
align.seqs(fasta=ML.trim.contigs.trim.unique.abund.fasta, reference=/mnt/matrix/symbio/db/silva.seed_v132.align, processors=20)

### Get sequence alignment info!
summary.seqs(fasta=ML.trim.contigs.trim.unique.abund.align)

### Remove unaligned
screen.seqs(fasta=ML.trim.contigs.trim.unique.abund.align, count=ML.trim.contigs.trim.abund.count_table, start=13862, end=23444, minlength=220)

### Remove gaps
filter.seqs(fasta=current, vertical=T, trump=.)

### Redo unique.seqs
unique.seqs(fasta=ML.trim.contigs.trim.unique.abund.good.filter.fasta, count=ML.trim.contigs.trim.abund.good.count_table)
    # ML.trim.contigs.trim.unique.abund.good.filter.count_table
    # ML.trim.contigs.trim.unique.abund.good.filter.unique.fasta


### Identify and remove chimaeric sequences using UChime
chimera.uchime(fasta=ML.trim.contigs.trim.unique.abund.good.filter.unique.fasta, reference=self, count=ML.trim.contigs.trim.unique.abund.good.filter.count_table, dereplicate=f, mindiv=0.35, processors=20, minh=0.5, xn=3)
remove.seqs(accnos=current, fasta=current, count=current)


### Classify the sequences by taxonomy
classify.seqs(fasta=current, count=current, reference=/mnt/matrix/symbio/db/silva.seed_v132.align, taxonomy=/mnt/matrix/symbio/db/silva.seed_v132.tax, cutoff=80)

remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-Archaea-Eukaryota)

summary.tax(taxonomy=current, count=current)

### Simplify file names
rename.file(fasta=current, count=current, taxonomy=current, prefix=ML_16S)

### Compute distance matrix, cluster, construct summary tables
dist.seqs(fasta=ML_16S.fasta, processors=24, cutoff=0.05)
cluster(column=current, count=ML_16S.count_table, cutoff=0.03, method=opti)
bin.seqs(list=current, fasta=current, label=0.03)
make.shared(list=current, count=current, label=0.03)


### Classifying OTUs
classify.otu(list=current,count=current,taxonomy=current, label=0.03)

### Extracting representative sequences
get.oturep(column=current, list=current, fasta=current, count=current, cutoff=0.01)



